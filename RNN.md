# Recurrent Neural Network（RNN）

**核心思想**

RNN的核心思想是在处理序列时，网络不仅考虑**当前输入**，还利用**之前的信息**

**工作原理**

具体来说：

1. 隐藏层接收**当前输入**$x_t$和前一时间步的隐藏状态$h_{t-1}$
2. 隐藏层计算得到新的**隐藏状态**$h_t$（这就是隐藏层的输出）
3. 这个隐藏层的输出$h_t$会：
   - 被存储为记忆并传递到下一时间步
   - 作为当前时间步的状态表示

**同一个Network在3个不同的时间点，被使用了3次**

![image-20250420134243810](C:\Users\86135\AppData\Roaming\Typora\typora-user-images\image-20250420134243810.png)

## Long Short-term Memory(LSTM)

**核心思想**

引入了**门控机制**和**细胞状态**，使网络能够更有效地学习长距离依赖关系

**核心组件**

1. 细胞状态 (Cell State)：
   - 贯穿整个LSTM单元的主要信息高速公路
   - 允许信息几乎不变地流过整个网络
   - 表示为$C_t$
2. 三种门控机制：
   - **遗忘门 (Forget Gate)**：决定丢弃细胞状态中的哪些信息
   - **输入门 (Input Gate)**：决定更新哪些新信息到细胞状态
   - **输出门 (Output Gate)**：决定基于细胞状态输出哪些信息
3. 隐藏状态 (Hidden State)：
   - 类似标准RNN的隐藏状态
   - 但受门控机制控制
   - 表示为$h_t$

**工作流程**

**1.  遗忘门**

遗忘门输出：$f(z_f) = σ(z_f)$

细胞状态：$C_{t-1} = h_{t-1}f(z_f)$

- 接收前一时间步的隐藏状态$h_{t-1}$
  - 遗忘门打开，保留之前的细胞状态
  - 遗忘门关闭，重置之前的细胞状态
- σ是sigmoid激活函数

**2.  输入门**

输入门输出：$f(z_i) = σ(z_i)$

- 输入门决定更新哪些信息
- 候选细胞状态提供新的候选值

**3.  更新细胞状态**

$C_t = C_{t-1}f(z_f) + g(z)f(z_i)$

- 旧细胞状态$C_{t-1}$与遗忘门输出相乘，丢弃不需要的信息
- 新的候选值经输入门调节后加入细胞状态
- 细胞状态更新后会进行输出

**4.  输出门和隐藏状态**

输出门输出：$f(z_o) = σ(z_o)$

- 输出门$o$决定输出细胞状态的哪些部分
- 细胞状态经tanh处理并与输出门相乘，生成最终隐藏状态

**5. 输入序列**

输入经过sigmoid的输出：$g(z) = σ(z)$

**6. 输出值**

$a = \sigma (C_t)f(z_o)$

![image-20250420150108029](C:\Users\86135\AppData\Roaming\Typora\typora-user-images\image-20250420150108029.png)

### 优势

**LSTM (长的短期记忆网络)** 被专门设计用来缓解RNN中的梯度消失问题。LSTM通过几个关键机制有效地处理了梯度消失：

1. **单元状态 (Cell State)** - LSTM引入了一条贯穿整个序列的"高速公路"，让信息可以直接从早期时间步传递到后期时间步，而不必经过多次非线性变换。这条路径上只有简单的线性操作（加法和乘法），使梯度能够更容易地反向流动。
2. **门控机制**\- LSTM有三个门：输入门、遗忘门和输出门。这些门控制信息的流动：
   - 遗忘门决定丢弃哪些信息
   - 输入门决定存储哪些新信息
   - 输出门决定输出哪些信息
3. **加法操作** - 与标准RNN中的乘法操作不同，LSTM使用加法来更新单元状态。由于加法操作的导数是1，不会导致梯度缩放，从而避免了梯度消失问题。
4. **选择性记忆机制** - LSTM可以学会长期保存重要信息，同时有选择地更新或丢弃不再需要的信息。

从数学角度看，标准RNN中梯度会依赖于权重的连乘，而LSTM中的单元状态更新**主要涉及加法操作（Memory和Input的值是相加的）**，反向传播时梯度不会因为连乘而衰减。

## Slot Filling（槽位填充）

**工作原理**

1. **槽位定义**：根据特定领域或任务预先定义一组槽位（如餐厅预订可能包括"菜系"、"人数"、"日期"、"时间"等槽位）
2. **序列标注**：通常将槽位填充建模为序列标注问题
   - 对输入句子中的每个词进行标注，指出它是否属于某个槽位



## 缺陷

### 梯度爆炸原因

1. **重复矩阵乘法**：在反向传播过程中，梯度会通过时间步骤反向传递，这涉及到重复的权重矩阵乘法。如果权重矩阵的特征值大于1，那么随着时间步的增加，梯度值会呈指数增长。
2. **长序列累积效应**：当处理长序列数据时，梯度需要通过多个时间步传播，每通过一步就会与权重矩阵相乘，如果权重较大，梯度会迅速累积膨胀。
3. **激活函数的导数**：如果使用的激活函数（如ReLU）在某些区域导数大于1，也会放大梯度。

### 梯度消失原因

1. **重复矩阵乘法**：与梯度爆炸类似，但当权重矩阵的特征值小于1时，重复相乘会导致梯度值逐渐变得非常小。
2. **饱和激活函数**：传统RNN常用的sigmoid和tanh激活函数，它们的导数在输入绝对值较大时会接近于0，进一步促进了梯度消失。
3. **长距离依赖问题**：对于长序列，早期时间步的信息需要经过多次传递才能影响当前输出，但梯度消失使得网络难以捕获这些长期依赖关系。

### Connectionist Temporal Classification（连接时序分类）

CTC是一个用于**序列建模和标注对齐**的**损失函数**

**作用**

在语音识别中，输入是一个很长的声学特征序列，而输出是一段文本（如“hello”）。传统方法需要知道每个字符对应声学特征的起止时间，即对齐信息。CTC 的关键就在于：

- **不需要对齐标注**，它自动学习从输入序列中找出最可能的输出序列。
- 允许重复字符和空白符 `blank`，通过规则对多个路径进行归一合并，从而输出最终的目标序列。、

**学习**

CTC Loss 给每一条“合法路径”分配一个概率，最后对所有合法路径求和得到一个总概率。模型学习的目标就是：让这些合法路径的概率之和最大化（也就是 loss 越小越好）。